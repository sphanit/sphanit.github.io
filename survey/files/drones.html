<!DOCTYPE html>
<html><head>
		<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
		<title>Zotero Report</title>
		<link rel="stylesheet" type="text/css" href="data:text/css;base64,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">
		<link rel="stylesheet" type="text/css" media="screen,projection" href="data:text/css;base64,LyogR2VuZXJpYyBzdHlsZXMgKi8KYm9keSB7Cglmb250OiA2Mi41JSBHZW9yZ2lhLCBUaW1lcywgc2VyaWY7Cgl3aWR0aDogNzgwcHg7CgltYXJnaW46IDAgYXV0bzsKfQoKaDIgewoJZm9udC1zaXplOiAxLjVlbTsKCWxpbmUtaGVpZ2h0OiAxLjVlbTsKCWZvbnQtZmFtaWx5OiBHZW9yZ2lhLCBUaW1lcywgc2VyaWY7Cn0KCnAgewoJbGluZS1oZWlnaHQ6IDEuNWVtOwp9CgphOmxpbmssIGE6dmlzaXRlZCB7Cgljb2xvcjogIzkwMDsKfQoKYTpob3ZlciwgYTphY3RpdmUgewoJY29sb3I6ICM3Nzc7Cn0KCgp1bC5yZXBvcnQgewoJZm9udC1zaXplOiAxLjRlbTsKCXdpZHRoOiA2ODBweDsKCW1hcmdpbjogMCBhdXRvOwoJcGFkZGluZzogMjBweCAyMHB4Owp9CgovKiBNZXRhZGF0YSB0YWJsZSAqLwp0YWJsZSB7Cglib3JkZXI6IDFweCAjY2NjIHNvbGlkOwoJb3ZlcmZsb3c6IGF1dG87Cgl3aWR0aDogMTAwJTsKCW1hcmdpbjogLjFlbSBhdXRvIC43NWVtOwoJcGFkZGluZzogMC41ZW07Cn0K">
		<link rel="stylesheet" type="text/css" media="print" href="data:text/css;base64,Ym9keSB7Cglmb250OiAxMnB0ICJUaW1lcyBOZXcgUm9tYW4iLCBUaW1lcywgR2VvcmdpYSwgc2VyaWY7CgltYXJnaW46IDA7Cgl3aWR0aDogYXV0bzsKCWNvbG9yOiBibGFjazsKfQoKLyogUGFnZSBCcmVha3MgKHBhZ2UtYnJlYWstaW5zaWRlIG9ubHkgcmVjb2duaXplZCBieSBPcGVyYSkgKi8KaDEsIGgyLCBoMywgaDQsIGg1LCBoNiB7CglwYWdlLWJyZWFrLWFmdGVyOiBhdm9pZDsKCXBhZ2UtYnJlYWstaW5zaWRlOiBhdm9pZDsKfQoKdWwsIG9sLCBkbCB7CglwYWdlLWJyZWFrLWluc2lkZTogYXZvaWQ7Cgljb2xvci1hZGp1c3Q6IGV4YWN0Owp9CgpoMiB7Cglmb250LXNpemU6IDEuM2VtOwoJbGluZS1oZWlnaHQ6IDEuM2VtOwp9CgphIHsKCWNvbG9yOiAjMDAwOwoJdGV4dC1kZWNvcmF0aW9uOiBub25lOwp9Cg==">
	</head>
	<body>
		<ul class="report combineChildItems">
			<li id="item_5DXS3JHV" class="item conferencePaper">
			<h2>Aerial Flight Paths for Communication: How Participants Perceive and Intend to Respond to Drone Movements</h2>
				<table>
					<tbody><tr>
						<th>Type</th>
						<td>Conference Paper</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Alisha Bevins</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Brittany A. Duncan</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1145/3434073.3444645">https://doi.org/10.1145/3434073.3444645</a></td>
					</tr>
					<tr>
					<th>Series</th>
						<td>HRI '21</td>
					</tr>
					<tr>
					<th>Place</th>
						<td>New York, NY, USA</td>
					</tr>
					<tr>
					<th>Publisher</th>
						<td>Association for Computing Machinery</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>16–23</td>
					</tr>
					<tr>
					<th>ISBN</th>
						<td>978-1-4503-8289-2</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>March 8, 2021</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1145/3434073.3444645">10.1145/3434073.3444645</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>11/30/2021, 1:00:00 AM</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>ACM Digital Library</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>This work has developed an iteratively refined understanding 
of participants' natural perceptions and responses to unmanned aerial 
vehicle (UAV) flight paths, or gestures. This includes both what they 
believe the UAV is trying to communicate to them, in addition to how 
they expect to respond through physical action. Previous work in this 
area has focused on eliciting gestures from participants to communicate 
specific states, or leveraging gestures that are observed in the world 
rather than on understanding what the participants believe is being 
communicated and how they would respond. This work investigates previous
 gestures either created or categorized by participants to understand 
the perceived content of their communication or expected response, 
through categories created by participant free responses and confirmed 
through forced choice testing. The human-robot interaction community can
 leverage this work to better understand how people perceive UAV flight 
paths, inform future designs for non-anthropomorphic robot 
communications, and apply lessons learned to elicit informative labels 
from people who may or may not be operating the vehicle. We found that 
the Negative Attitudes towards Robots Scale (NARS) can be a good 
indicator of how we can expect a person to react to a robot. 
Recommendations are also provided to use motion approaching/retreating 
from a person to encourage following, perpendicular to their field of 
view for blocking, and to use either no motion or large altitude changes
 to encourage viewing.</td>
					</tr>
					<tr>
					<th>Proceedings Title</th>
						<td>Proceedings of the 2021 ACM/IEEE International Conference on Human-Robot Interaction</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Aerial Flight Paths for Communication</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>11/30/2021, 3:37:41 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>11/30/2021, 3:37:41 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>drones</li>
					<li>gestures</li>
					<li>motion</li>
					<li>uav</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_6QDU5MGR">Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_IWESBQXU" class="item conferencePaper">
			<h2>Aerial social force model: A new framework to accompany people using autonomous flying robots</h2>
				<table>
					<tbody><tr>
						<th>Type</th>
						<td>Conference Paper</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>A. Garrell</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Luis Garza-Elizondo</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>M. Villamizar</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>F. Herrero</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>A. Sanfeliu</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://ieeexplore.ieee.org/document/8206627/">http://ieeexplore.ieee.org/document/8206627/</a></td>
					</tr>
					<tr>
					<th>Place</th>
						<td>Vancouver, BC</td>
					</tr>
					<tr>
					<th>Publisher</th>
						<td>IEEE</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>7011-7017</td>
					</tr>
					<tr>
					<th>ISBN</th>
						<td>978-1-5386-2682-5</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>9/2017</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1109/IROS.2017.8206627">10.1109/IROS.2017.8206627</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>4/1/2021, 10:00:58 AM</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>DOI.org (Crossref)</td>
					</tr>
					<tr>
					<th>Conference Name</th>
						<td>2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>In this paper, we propose a novel Aerial Social Force Model 
(ASFM) that allows autonomous ﬂying robots to accompany humans in urban 
environments in a safe and comfortable manner. To date, we are not aware
 of other stateof-the-art method that accomplish this task. The proposed
 approach is a 3D version of the Social Force Model (SFM) for the ﬁeld 
of aerial robots which includes an interactive human-robot navigation 
scheme capable of predicting human motions and intentions so as to 
safely accompany them to their ﬁnal destination. ASFM also introduces a 
new metric to ﬁnetune the parameters of the force model, and to evaluate
 the performance of the aerial robot companion based on comfort and 
distance between the robot and humans. The presented approach is 
extensively validated in diverse simulations and real experiments, and 
compared against other similar works in the literature. ASFM attains 
remarkable results and shows that it is a valuable framework for social 
robotics applications, such as guiding people or human-robot 
interaction.</td>
					</tr>
					<tr>
					<th>Proceedings Title</th>
						<td>2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Aerial social force model</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/1/2021, 10:00:58 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/1/2021, 10:00:59 AM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_YE46JP4F">Garrell et al. - 2017 - Aerial social force model A new framework to acco.pdf					</li>
				</ul>
			</li>


			<li id="item_V84MDC3D" class="item conferencePaper">
			<h2>Comfortable approach distance with small Unmanned Aerial Vehicles</h2>
				<table>
					<tbody><tr>
						<th>Type</th>
						<td>Conference Paper</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Brittany A. Duncan</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Robin R. Murphy</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://ieeexplore.ieee.org/document/6628409/">http://ieeexplore.ieee.org/document/6628409/</a></td>
					</tr>
					<tr>
					<th>Place</th>
						<td>Gyeongju</td>
					</tr>
					<tr>
					<th>Publisher</th>
						<td>IEEE</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>786-792</td>
					</tr>
					<tr>
					<th>ISBN</th>
						<td>978-1-4799-0509-6</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>08/2013</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1109/ROMAN.2013.6628409">10.1109/ROMAN.2013.6628409</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>11/30/2021, 3:35:55 PM</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>DOI.org (Crossref)</td>
					</tr>
					<tr>
					<th>Conference Name</th>
						<td>2013 IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN)</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>This paper presents the ﬁrst known humansubject study of 
comfortable approach distance and height for human interaction with a 
small unmanned aerial vehicle (sUAV), ﬁnding no conclusive difference in
 comfort with a sUAV approaching a human at above head height or below 
head height. Understanding the amount, if any, of discomfort introduced 
by a sUAV ﬂying in close proximity to a human is critical for law 
enforcement, crowd control, entertainment, or ﬂying personal assistants.
 Previous work has focused on how humans interact with each other or 
with unmanned ground vehicles, and the experimental methods typically 
rely on the human participant to consciously express distress. The 
approach taken was to duplicate the experimental set up in human 
proxemics studies, while adding psychophysiological sensing, under the 
hypothesis that human-robot interaction will mirror human-human 
interaction. The 16 participant, within-subjects experiment did not 
conﬁrm this hypothesis. Instead a sUAV above height of a “tall” person 
in human experiments (2.13 m) did not produce statistically different 
heart rate variability nor cause the participant to stop the robot 
further away than for a sUAV at a “short” height (1.52 m). The lack of 
effect may be due to two possible confounds: i) duplicating prior human 
proxemics experiments did not capture how a sUAV would likely move or 
interact and ii) telling the participants that the robot could not hurt 
them. Despite possible confounding, the results raise the question of 
whether humanhuman psychological and physical distancing behavior 
transfers to human-aerial robot interactions.</td>
					</tr>
					<tr>
					<th>Proceedings Title</th>
						<td>2013 IEEE RO-MAN</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>11/30/2021, 3:35:55 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>11/30/2021, 3:35:55 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_XQLLN2JZ">Duncan and Murphy - 2013 - Comfortable approach distance with small Unmanned .pdf					</li>
				</ul>
			</li>


			<li id="item_2ACYEHT7" class="item conferencePaper">
			<h2>Drone &amp; me: an exploration into natural human-drone interaction</h2>
				<table>
					<tbody><tr>
						<th>Type</th>
						<td>Conference Paper</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jessica R. Cauchard</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jane L. E</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Kevin Y. Zhai</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>James A. Landay</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://dl.acm.org/citation.cfm?doid=2750858.2805823">http://dl.acm.org/citation.cfm?doid=2750858.2805823</a></td>
					</tr>
					<tr>
					<th>Place</th>
						<td>Osaka, Japan</td>
					</tr>
					<tr>
					<th>Publisher</th>
						<td>ACM Press</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>361-365</td>
					</tr>
					<tr>
					<th>ISBN</th>
						<td>978-1-4503-3574-4</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2015</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1145/2750858.2805823">10.1145/2750858.2805823</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>11/30/2021, 3:42:23 PM</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>DOI.org (Crossref)</td>
					</tr>
					<tr>
					<th>Conference Name</th>
						<td>the 2015 ACM International Joint Conference</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Personal drones are becoming popular. It is challenging to 
design how to interact with these flying robots. We present a 
Wizard-of-Oz (WoZ) elicitation study that informs how to naturally 
interact with drones. Results show strong agreement between participants
 for many interaction techniques, as when gesturing for the drone to 
stop. We discovered that people interact with drones as with a person or
 a pet, using interpersonal gestures, such as beckoning the drone 
closer. We detail the interaction metaphors observed and offer design 
insights for human-drone interactions.</td>
					</tr>
					<tr>
					<th>Proceedings Title</th>
						<td>Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing - UbiComp '15</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Drone &amp; me</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>11/30/2021, 3:42:23 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>11/30/2021, 3:42:24 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_K2UFEWV9">Cauchard et al. - 2015 - Drone &amp; me an exploration into natural human-dron.pdf					</li>
				</ul>
			</li>


			<li id="item_TW2FPHMS" class="item conferencePaper">
			<h2>Exploring Proxemics for Human-Drone Interaction</h2>
				<table>
					<tbody><tr>
						<th>Type</th>
						<td>Conference Paper</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Alexander Yeh</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Photchara Ratsamee</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Kiyoshi Kiyokawa</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yuki Uranishi</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Tomohiro Mashita</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Haruo Takemura</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Morten Fjeld</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Mohammad Obaid</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://dl.acm.org/doi/10.1145/3125739.3125773">https://dl.acm.org/doi/10.1145/3125739.3125773</a></td>
					</tr>
					<tr>
					<th>Place</th>
						<td>Bielefeld Germany</td>
					</tr>
					<tr>
					<th>Publisher</th>
						<td>ACM</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>81-88</td>
					</tr>
					<tr>
					<th>ISBN</th>
						<td>978-1-4503-5113-3</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2017-10-27</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1145/3125739.3125773">10.1145/3125739.3125773</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>11/30/2021, 3:34:52 PM</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>DOI.org (Crossref)</td>
					</tr>
					<tr>
					<th>Conference Name</th>
						<td>HAI '17: The Fifth International Conference on Human-Agent Interaction</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>We present a human-centered designed social drone aiming to be
 used in a human crowd environment. Based on design studies and focus 
groups, we created a prototype of a social drone with a social shape, 
face and voice for human interaction. We used the prototype for a 
proxemic study, comparing the required distance from the drone humans 
could comfortably accept compared with what they would require for a 
nonsocial drone. The social shaped design with greeting voice added 
decreased the acceptable distance markedly, as did present or previous 
pet ownership, and maleness. We also explored the proximity sphere 
around humans with a social shaped drone based on a validation study 
with variation of lateral distance and heights. Both lateral distance 
and the higher height of 1.8 m compared to the lower height of 1.2 m 
decreased the required comfortable distance as it approached.</td>
					</tr>
					<tr>
					<th>Proceedings Title</th>
						<td>Proceedings of the 5th International Conference on Human Agent Interaction</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>11/30/2021, 3:34:52 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>11/30/2021, 3:34:52 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_CM6Y5BK7">Yeh et al. - 2017 - Exploring Proxemics for Human-Drone Interaction.pdf					</li>
				</ul>
			</li>


			<li id="item_AGPZLH99" class="item bookSection">
			<h2>Knowing You, Seeing Me: Investigating User Preferences in Drone-Human Acknowledgement</h2>
				<table>
					<tbody><tr>
						<th>Type</th>
						<td>Book Section</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Walther Jensen</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Simon Hansen</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Hendrik Knoche</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1145/3173574.3173939">https://doi.org/10.1145/3173574.3173939</a></td>
					</tr>
					<tr>
					<th>Place</th>
						<td>New York, NY, USA</td>
					</tr>
					<tr>
					<th>Publisher</th>
						<td>Association for Computing Machinery</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>1–12</td>
					</tr>
					<tr>
					<th>ISBN</th>
						<td>978-1-4503-5620-6</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>April 21, 2018</td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>11/30/2021, 1:00:00 AM</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>ACM Digital Library</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>In the past, human proxemics research has poorly predicted 
human robot interaction distances. This paper presents three studies on 
drone gestures to acknowledge human presence and clarify suitable 
acknowledging distances. We evaluated four drone gestures based on 
non-verbal human greetings. The gestures included orienting towards the 
counterpart and salutation gestures. We tested these individually and in
 combination to create a feeling of acknowledgement in people. Our users
 preferred being acknowledged from two meters away but gestures were 
also effective from four meters. Rotating the drone towards the user 
elicited a higher degree of acknowledgement than without. We conclude 
with a set design guidelines for drone gestures.</td>
					</tr>
					<tr>
					<th>Book Title</th>
						<td>Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Knowing You, Seeing Me</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>11/30/2021, 3:36:51 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>11/30/2021, 3:36:51 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>acknowledgement</li>
					<li>drone</li>
					<li>drone gesture</li>
					<li>hdi</li>
					<li>hri</li>
					<li>human-drone interaction</li>
					<li>proxemics</li>
					<li>quadcopter</li>
					<li>uav</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_6W7ZLD6M">Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_GLJ9DKFX" class="item journalArticle">
			<h2>Probabilistically Safe Robot Planning with Confidence-Based Human Predictions</h2>
				<table>
					<tbody><tr>
						<th>Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jaime F. Fisac</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Andrea Bajcsy</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sylvia L. Herbert</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>David Fridovich-Keil</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Steven Wang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Claire J. Tomlin</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Anca D. Dragan</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/1806.00109">http://arxiv.org/abs/1806.00109</a></td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>arXiv:1806.00109 [cs]</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2018-05-31</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv: 1806.00109</td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>11/4/2019, 1:10:57 PM</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>In order to safely operate around humans, robots can employ 
predictive models of human motion. Unfortunately, these models cannot 
capture the full complexity of human behavior and necessarily introduce 
simplifying assumptions. As a result, predictions may degrade whenever 
the observed human behavior departs from the assumed structure, which 
can have negative implications for safety. In this paper, we observe 
that how "rational" human actions appear under a particular model can be
 viewed as an indicator of that model's ability to describe the human's 
current motion. By reasoning about this model confidence in a real-time 
Bayesian framework, we show that the robot can very quickly modulate its
 predictions to become more uncertain when the model performs poorly. 
Building on recent work in provably-safe trajectory planning, we 
leverage these confidence-aware human motion predictions to generate 
assured autonomous robot motion. Our new analysis combines worst-case 
tracking error guarantees for the physical robot with probabilistic 
time-varying human predictions, yielding a quantitative, probabilistic 
safety certificate. We demonstrate our approach with a quadcopter 
navigating around a human.</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>11/12/2019, 4:52:56 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>11/12/2019, 4:52:56 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Machine Learning</li>
					<li>Computer Science - Robotics</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_SS2X76DE">
<p class="plaintext">Comment: Robotics Science and Systems (RSS) 2018</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_KU836D88">arXiv Fulltext PDF					</li>
					<li id="item_IZNKPTRR">arXiv.org Snapshot					</li>
				</ul>
			</li>


			<li id="item_4WN5AP25" class="item journalArticle">
			<h2>Social-aware drone navigation using social force model</h2>
				<table>
					<tbody><tr>
						<th>Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Garza Elizondo</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Luis Alberto</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://upcommons.upc.edu/handle/2117/104437">https://upcommons.upc.edu/handle/2117/104437</a></td>
					</tr>
					<tr>
					<th>Rights</th>
						<td>Open Access</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2016-10-07</td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>11/4/2019, 12:52:58 PM</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>upcommons.upc.edu</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>eng</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>11/12/2019, 4:52:56 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>11/12/2019, 4:52:56 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Àrees temàtiques de la UPC::Informàtica</li>
					<li>Avions no tripulats</li>
					<li>Collision avoidance</li>
					<li>Drone aircraft</li>
					<li>Human-Drone Interaction</li>
					<li>Interacció Humà-Drone</li>
					<li>Navegació Social</li>
					<li>Prevenció de col·lisions</li>
					<li>Social navigation</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_A2R5P8LH">Full Text PDF					</li>
					<li id="item_3MSMYEM4">Snapshot					</li>
				</ul>
			</li>


			<li id="item_CCG7SUUM" class="item journalArticle">
			<h2>Socially Aware Path Planning for a Flying Robot in Close Proximity of Humans</h2>
				<table>
					<tbody><tr>
						<th>Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Hyung-Jin Yoon</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Christopher Widdowson</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Thiago Marinho</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ranxiao Frances Wang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Naira Hovakimyan</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1145/3341570">https://doi.org/10.1145/3341570</a></td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>3</td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>4</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>41:1–41:24</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>ACM Transactions on Cyber-Physical Systems</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2378-962X</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>September 4, 2019</td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>ACM Trans. Cyber-Phys. Syst.</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1145/3341570">10.1145/3341570</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>11/30/2021, 3:39:01 PM</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>October 2019</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>In this article, we present a preliminary motion planning 
framework for a cyber-physical system consisting of a human and a flying
 robot in vicinity. The motion planning of the flying robot takes into 
account the human’s safety perception. We aim to determine a parametric 
model for the human’s safety perception based on test data. We use 
virtual reality as a safe testing environment to collect safety 
perception data reflected on galvanic skin response (GSR) from the test 
subjects experiencing a flying robot in their vicinity. The GSR signal 
contains both meaningful information driven by the interaction with the 
robot and also disturbances from unknown factors. To address the issue, 
we use two parametric models to approximate the GSR data: (1) a function
 of the robot’s position and velocity and (2) a random distribution. 
Intuitively, we need to choose the more likely model given the data. 
When GSR is statistically independent of the flying robot, then the 
random distribution should be selected instead of the function of the 
robot’s position and velocity. We implement the intuitive idea under the
 framework of hidden Markov model (HMM) estimation. As a result, the 
proposed HMM-based model improves the likelihood compared to the 
Gaussian noise model, which does not make a distinction between relevant
 and irrelevant samples due to unknown factors. We also present a 
numerical optimal path planning method that considers the safety 
perception model while ensuring spatial separation from the obstacle 
despite the time discretization. Optimal paths generated using the 
proposed model result in a reasonably safe distance from the human. In 
contrast, the trajectories generated by the standard regression model 
with the Gaussian noise assumption, without consideration of unknown 
factors, have undesirable shapes.</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>11/30/2021, 3:39:01 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>11/30/2021, 3:39:02 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>hidden Markov model</li>
					<li>Human-robot interaction</li>
					<li>optimal path planning</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_SHNG8KNC">Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_4C9HZWW8" class="item conferencePaper">
			<h2>Teaching a Drone to Accompany a Person from Demonstrations using Non-Linear ASFM</h2>
				<table>
					<tbody><tr>
						<th>Type</th>
						<td>Conference Paper</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Anais Garrell</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Carles Coll</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Rene Alquezar</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Alberto Sanfeliu</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://ieeexplore.ieee.org/document/8967675/">https://ieeexplore.ieee.org/document/8967675/</a></td>
					</tr>
					<tr>
					<th>Place</th>
						<td>Macau, China</td>
					</tr>
					<tr>
					<th>Publisher</th>
						<td>IEEE</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>1985-1991</td>
					</tr>
					<tr>
					<th>ISBN</th>
						<td>978-1-72814-004-9</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>11/2019</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1109/IROS40897.2019.8967675">10.1109/IROS40897.2019.8967675</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>11/30/2021, 3:41:19 PM</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>DOI.org (Crossref)</td>
					</tr>
					<tr>
					<th>Conference Name</th>
						<td>2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>In this paper, we present a new method based on the Aerial 
Social Force Model (ASFM) to allow human-drone side-by-side social 
navigation in real environments. To tackle this problem, the present 
work proposes a new nonlinear-based approach using Neural Networks. To 
learn and test the rightness of the new approach, we built a new dataset
 with simulated environments and we recorded motion controls provided by
 a human expert tele-operating the drone. The recorded data is then used
 to train a neural network which maps interaction forces to acceleration
 commands. The system is also reinforced with a human path prediction 
module to improve the drone’s navigation, as well as, a collision 
detection module to completely avoid possible impacts. Moreover, a 
performance metric is deﬁned which allows us to numerically evaluate and
 compare the fulﬁllment of the different learned policies. The method 
was validated by a large set of simulations; we also conducted real-life
 experiments with an autonomous drone to verify the framework described 
for the navigation process. In addition, a user study has been realized 
to reveal the social acceptability of the method.</td>
					</tr>
					<tr>
					<th>Proceedings Title</th>
						<td>2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>11/30/2021, 3:41:19 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>11/30/2021, 3:41:19 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_6FUIKN4T">Garrell et al. - 2019 - Teaching a Drone to Accompany a Person from Demons.pdf					</li>
				</ul>
			</li>

		</ul>
	
</body></html>