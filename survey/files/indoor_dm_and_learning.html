<!DOCTYPE html>
<html><head>
		<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
		<title>Zotero Report</title>
		<link rel="stylesheet" type="text/css" href="data:text/css;base64,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">
		<link rel="stylesheet" type="text/css" media="screen,projection" href="data:text/css;base64,LyogR2VuZXJpYyBzdHlsZXMgKi8KYm9keSB7Cglmb250OiA2Mi41JSBHZW9yZ2lhLCBUaW1lcywgc2VyaWY7Cgl3aWR0aDogNzgwcHg7CgltYXJnaW46IDAgYXV0bzsKfQoKaDIgewoJZm9udC1zaXplOiAxLjVlbTsKCWxpbmUtaGVpZ2h0OiAxLjVlbTsKCWZvbnQtZmFtaWx5OiBHZW9yZ2lhLCBUaW1lcywgc2VyaWY7Cn0KCnAgewoJbGluZS1oZWlnaHQ6IDEuNWVtOwp9CgphOmxpbmssIGE6dmlzaXRlZCB7Cgljb2xvcjogIzkwMDsKfQoKYTpob3ZlciwgYTphY3RpdmUgewoJY29sb3I6ICM3Nzc7Cn0KCgp1bC5yZXBvcnQgewoJZm9udC1zaXplOiAxLjRlbTsKCXdpZHRoOiA2ODBweDsKCW1hcmdpbjogMCBhdXRvOwoJcGFkZGluZzogMjBweCAyMHB4Owp9CgovKiBNZXRhZGF0YSB0YWJsZSAqLwp0YWJsZSB7Cglib3JkZXI6IDFweCAjY2NjIHNvbGlkOwoJb3ZlcmZsb3c6IGF1dG87Cgl3aWR0aDogMTAwJTsKCW1hcmdpbjogLjFlbSBhdXRvIC43NWVtOwoJcGFkZGluZzogMC41ZW07Cn0K">
		<link rel="stylesheet" type="text/css" media="print" href="data:text/css;base64,Ym9keSB7Cglmb250OiAxMnB0ICJUaW1lcyBOZXcgUm9tYW4iLCBUaW1lcywgR2VvcmdpYSwgc2VyaWY7CgltYXJnaW46IDA7Cgl3aWR0aDogYXV0bzsKCWNvbG9yOiBibGFjazsKfQoKLyogUGFnZSBCcmVha3MgKHBhZ2UtYnJlYWstaW5zaWRlIG9ubHkgcmVjb2duaXplZCBieSBPcGVyYSkgKi8KaDEsIGgyLCBoMywgaDQsIGg1LCBoNiB7CglwYWdlLWJyZWFrLWFmdGVyOiBhdm9pZDsKCXBhZ2UtYnJlYWstaW5zaWRlOiBhdm9pZDsKfQoKdWwsIG9sLCBkbCB7CglwYWdlLWJyZWFrLWluc2lkZTogYXZvaWQ7Cgljb2xvci1hZGp1c3Q6IGV4YWN0Owp9CgpoMiB7Cglmb250LXNpemU6IDEuM2VtOwoJbGluZS1oZWlnaHQ6IDEuM2VtOwp9CgphIHsKCWNvbG9yOiAjMDAwOwoJdGV4dC1kZWNvcmF0aW9uOiBub25lOwp9Cg==">
	</head>
	<body>
		<ul class="report combineChildItems">
			<li id="item_EFPJHWXL" class="item journalArticle">
			<h2>A Data-Driven Framework for Proactive Intention-Aware Motion Planning of a Robot in a Human Environment</h2>
				<table>
					<tbody><tr>
						<th>Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Rahul Peddi</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Carmelo Di Franco</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>SHIJIE Gao</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Nicola Bezzo</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>7</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Zotero</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>For safe and efﬁcient human-robot interaction, a robot needs 
to predict and understand the intentions of humans who share the same 
space. Mobile robots are traditionally built to be reactive, moving in 
unnatural ways without following social protocol, hence forcing people 
to behave very differently from human-human interaction rules, which can
 be overcome if robots instead were proactive. In this paper, we build 
an intention-aware proactive motion planning strategy for mobile robots 
that coexist with multiple humans. We propose a framework that uses 
Hidden Markov Model (HMM) theory with a history of observations to: i) 
predict future states and estimate the likelihood that humans will cross
 the path of a robot, and ii) concurrently learn, update, and improve 
the predictive model with new observations at run-time. Stochastic 
reachability analysis is proposed to identify multiple possibilities of 
future states and a control scheme that leverages temporal virtual 
physics inspired by spring-mass systems is proposed to enable safe 
proactive motion planning. The proposed approach is validated with 
simulations and experiments involving an unmanned ground vehicle (UGV) 
performing go-to-goal operations in the presence of multiple humans, 
demonstrating improved performance and effectiveness of online learning 
when compared to reactive obstacle avoidance approaches.</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/14/2021, 11:49:47 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/14/2021, 11:49:47 AM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_3S922Z48">Peddi et al. - A Data-Driven Framework for Proactive Intention-Aw.pdf					</li>
				</ul>
			</li>


			<li id="item_T3EQM32F" class="item journalArticle">
			<h2>An Integrative Approach of Social Dynamic Long Short-Term Memory 
and Deep Reinforcement Learning for Socially Aware Robot Navigation</h2>
				<table>
					<tbody><tr>
						<th>Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Xuan Tung Truong</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Trung Dung Ngo</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>2</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Zotero</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>In this study, we propose an integrative approach of Long 
short-term memory (LSTM) networks and a deep reinforcement learning 
(DRL) technique for socially aware robot navigation in crowded and 
dynamic environments. The proposed system is an integration of two main 
stages: (1) socio-spatio-temporal characteristics of the humans are 
encoded by using the LSTM networks, and (2) the encoded social dynamic 
characteristics are then fed into the DRL algorithm in order to generate
 motion control commands for a mobile robot. We integrated the developed
 system onto the conventional mobile robot navigation system and veriﬁed
 it in a simulated environment. The simulation results show that the 
proposed socially aware robot navigation system enables the mobile robot
 to behave in socially acceptable manners.</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>11/12/2019, 4:53:31 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>11/12/2019, 4:53:31 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_Q6TZIATS">Truong and Ngo - An Integrative Approach of Social Dynamic Long Sho.pdf					</li>
				</ul>
			</li>


			<li id="item_LAAHU8HU" class="item journalArticle">
			<h2>Crowd-Robot Interaction: Crowd-aware Robot Navigation with Attention-based Deep Reinforcement Learning</h2>
				<table>
					<tbody><tr>
						<th>Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Changan Chen</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yuejiang Liu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sven Kreiss</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Alexandre Alahi</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/1809.08835">http://arxiv.org/abs/1809.08835</a></td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>arXiv:1809.08835 [cs]</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2019-02-19</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv: 1809.08835</td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>11/4/2019, 11:09:15 AM</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Mobility in an effective and socially-compliant manner is an 
essential yet challenging task for robots operating in crowded spaces. 
Recent works have shown the power of deep reinforcement learning 
techniques to learn socially cooperative policies. However, their 
cooperation ability deteriorates as the crowd grows since they typically
 relax the problem as a one-way Human-Robot interaction problem. In this
 work, we want to go beyond first-order Human-Robot interaction and more
 explicitly model Crowd-Robot Interaction (CRI). We propose to (i) 
rethink pairwise interactions with a self-attention mechanism, and (ii) 
jointly model Human-Robot as well as Human-Human interactions in the 
deep reinforcement learning framework. Our model captures the 
Human-Human interactions occurring in dense crowds that indirectly 
affects the robot's anticipation capability. Our proposed attentive 
pooling mechanism learns the collective importance of neighboring humans
 with respect to their future states. Various experiments demonstrate 
that our model can anticipate human dynamics and navigate in crowds with
 time efficiency, outperforming state-of-the-art methods.</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Crowd-Robot Interaction</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>11/12/2019, 4:53:31 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>11/12/2019, 4:53:31 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Machine Learning</li>
					<li>Computer Science - Robotics</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_TTA66QYU">
<p class="plaintext">Comment: Accepted at ICRA2019. Copyright may be transferred without notice, after which this version may no longer be accessible</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_JLVXYHYN">arXiv Fulltext PDF					</li>
					<li id="item_6ZJZFS3X">arXiv.org Snapshot					</li>
				</ul>
			</li>


			<li id="item_JXEVUCH7" class="item journalArticle">
			<h2>Graph Neural Networks for Human-aware Social Navigation</h2>
				<table>
					<tbody><tr>
						<th>Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Luis J. Manso</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ronit R. Jorvekar</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Diego R. Faria</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Pablo Bustos</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Pilar Bachiller</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/1909.09003">http://arxiv.org/abs/1909.09003</a></td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>arXiv:1909.09003 [cs]</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2019-09-19</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv: 1909.09003</td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>11/4/2019, 11:32:17 AM</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Autonomous navigation is a key skill for assistive and service
 robots. To be successful, robots have to navigate avoiding going 
through the personal spaces of the people surrounding them. Complying 
with social rules such as not getting in the middle of human-to-human 
and human-to-object interactions is also important. This paper suggests 
using Graph Neural Networks to model how inconvenient the presence of a 
robot would be in a particular scenario according to learned human 
conventions so that it can be used by path planning algorithms. To do 
so, we propose two ways of modelling social interactions using graphs 
and benchmark them with different Graph Neural Networks using the 
SocNav1 dataset. We achieve close-to-human performance in the dataset 
and argue that, in addition to promising results, the main advantage of 
the approach is its scalability in terms of the number of social factors
 that can be considered and easily embedded in code, in comparison with 
model-based approaches. The code used to train and test the resulting 
graph neural network is available in a public repository.</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>11/12/2019, 4:53:31 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>11/12/2019, 4:53:31 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Machine Learning</li>
					<li>Computer Science - Robotics</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_4WG4IYRM">
<p class="plaintext">Comment: Submitted to RA-L / ICRA2020</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_D5IYILRV">arXiv Fulltext PDF					</li>
					<li id="item_64N6WCEB">arXiv.org Snapshot					</li>
				</ul>
			</li>


			<li id="item_42AL6LJ7" class="item journalArticle">
			<h2>Human-Aware Robot Navigation by Long-Term Movement Prediction</h2>
				<table>
					<tbody><tr>
						<th>Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Lilli Bruckschen</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Kira Bungert</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Nils Dengler</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Maren Bennewitz</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>6</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Zotero</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Foresighted, human-aware navigation is a prerequisite for 
service robots acting in indoor environments. In this paper, we present a
 novel human-aware navigation approach that relies on long-term 
prediction of human movements. In particular, we consider the problem of
 ﬁnding a path from the robot’s current position to the initially 
unknown navigation goal of a moving user to provide timely assistance 
there. The navigation strategy has to minimize the robot’s arrival time 
and at the same time comply with the user’s comfort during the movement.
 Our solution predicts the user’s navigation goal based on the robot’s 
observations and prior knowledge about typical human transitions between
 objects. Based on the motion prediction, we then compute a 
time-dependent cost map that encodes the belief about the user’s 
positions at future time steps. Using this map, we solve the 
time-dependent shortest path problem to ﬁnd an efﬁcient path for the 
robot, which still abides by the rules of human comfort. To identify 
robot navigation actions that are perceived as uncomfortable by humans, 
we performed user surveys and deﬁned the corresponding constraints. We 
thoroughly evaluated our navigation system in simulation as well as in 
real-world experiments. As the results show, our system outperforms 
existing approaches in terms of human comfort, while still minimizing 
arrival times of the robot.</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/14/2021, 11:50:48 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/14/2021, 11:50:48 AM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_LHIAUPDX">Bruckschen et al. - Human-Aware Robot Navigation by Long-Term Movement.pdf					</li>
				</ul>
			</li>


			<li id="item_5CA3HB7L" class="item journalArticle">
			<h2>Learning Human Navigation Behavior Using Measured Human Trajectories in Crowded Spaces</h2>
				<table>
					<tbody><tr>
						<th>Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Muhammad Fahad</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Guang Yang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yi Guo</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>7</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Zotero</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>As humans and mobile robots increasingly coexist in public 
spaces, their close proximity demands that robots navigate following 
navigation strategies similar to those exhibited by humans. This could 
be achieved by learning directly from human demonstration trajectories 
in a machine learning framework. In this paper, we present a method to 
learn human navigation behaviors using an imitation learning approach 
based on generative adversarial imitation learning (GAIL), which has the
 ability of directly extracting navigation policy. Speciﬁcally, we use a
 large open human trajectory dataset that was experimentally collected 
in a crowded public space. We then recreate these human trajectories in a
 3D robotic simulator, and generate demonstration data using a LIDAR 
sensor onboard a robot with the robot following the measured human 
trajectories. We then propose a GAIL based algorithm, which uses 
occupancy maps generated using LIDAR data as the input, and outputs the 
navigation policy for robot navigation. Simulation experiments are 
conducted, and performance evaluation shows that the learned navigation 
policy generates trajectories qualitatively and quantitatively similar 
to human trajectories. Compared with existing works using analytical 
models (such as social force model) to generate human demonstration 
trajectories, our method learns directly from intrinsic human 
trajectories, thus exhibits more humanlike navigation behaviors.</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/14/2021, 11:50:55 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/14/2021, 11:50:55 AM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_XU9D9KL5">Fahad et al. - Learning Human Navigation Behavior Using Measured .pdf					</li>
				</ul>
			</li>


			<li id="item_B3I2ZLFJ" class="item journalArticle">
			<h2>Learning Local Planners for Human-Aware Navigation in Indoor Environments</h2>
				<table>
					<tbody><tr>
						<th>Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ronja Gã</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>8</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Zotero</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Established indoor robot navigation frameworks build on the 
separation between global and local planners. Whereas global planners 
rely on traditional graph search algorithms, local planners are expected
 to handle driving dynamics and resolve minor conﬂicts. We present a 
system to train neuralnetwork policies for such a local planner 
component, explicitly accounting for humans navigating the space. 
DRL-agents are trained in randomized virtual 2D environments with 
simulated human interaction. The trained agents can be deployed as a 
drop-in replacement for other local planners and signiﬁcantly improve on
 traditional implementations. Performance is demonstrated on a MiR-100 
transport robot.</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/14/2021, 11:50:58 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/14/2021, 11:50:58 AM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_HG9I44ZV">Gã - Learning Local Planners for Human-Aware Navigation.pdf					</li>
				</ul>
			</li>

		</ul>
	
</body></html>