<!DOCTYPE html>
<html><head>
		<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
		<title>Zotero Report</title>
		<link rel="stylesheet" type="text/css" href="data:text/css;base64,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">
		<link rel="stylesheet" type="text/css" media="screen,projection" href="data:text/css;base64,LyogR2VuZXJpYyBzdHlsZXMgKi8KYm9keSB7Cglmb250OiA2Mi41JSBHZW9yZ2lhLCBUaW1lcywgc2VyaWY7Cgl3aWR0aDogNzgwcHg7CgltYXJnaW46IDAgYXV0bzsKfQoKaDIgewoJZm9udC1zaXplOiAxLjVlbTsKCWxpbmUtaGVpZ2h0OiAxLjVlbTsKCWZvbnQtZmFtaWx5OiBHZW9yZ2lhLCBUaW1lcywgc2VyaWY7Cn0KCnAgewoJbGluZS1oZWlnaHQ6IDEuNWVtOwp9CgphOmxpbmssIGE6dmlzaXRlZCB7Cgljb2xvcjogIzkwMDsKfQoKYTpob3ZlciwgYTphY3RpdmUgewoJY29sb3I6ICM3Nzc7Cn0KCgp1bC5yZXBvcnQgewoJZm9udC1zaXplOiAxLjRlbTsKCXdpZHRoOiA2ODBweDsKCW1hcmdpbjogMCBhdXRvOwoJcGFkZGluZzogMjBweCAyMHB4Owp9CgovKiBNZXRhZGF0YSB0YWJsZSAqLwp0YWJsZSB7Cglib3JkZXI6IDFweCAjY2NjIHNvbGlkOwoJb3ZlcmZsb3c6IGF1dG87Cgl3aWR0aDogMTAwJTsKCW1hcmdpbjogLjFlbSBhdXRvIC43NWVtOwoJcGFkZGluZzogMC41ZW07Cn0K">
		<link rel="stylesheet" type="text/css" media="print" href="data:text/css;base64,Ym9keSB7Cglmb250OiAxMnB0ICJUaW1lcyBOZXcgUm9tYW4iLCBUaW1lcywgR2VvcmdpYSwgc2VyaWY7CgltYXJnaW46IDA7Cgl3aWR0aDogYXV0bzsKCWNvbG9yOiBibGFjazsKfQoKLyogUGFnZSBCcmVha3MgKHBhZ2UtYnJlYWstaW5zaWRlIG9ubHkgcmVjb2duaXplZCBieSBPcGVyYSkgKi8KaDEsIGgyLCBoMywgaDQsIGg1LCBoNiB7CglwYWdlLWJyZWFrLWFmdGVyOiBhdm9pZDsKCXBhZ2UtYnJlYWstaW5zaWRlOiBhdm9pZDsKfQoKdWwsIG9sLCBkbCB7CglwYWdlLWJyZWFrLWluc2lkZTogYXZvaWQ7Cgljb2xvci1hZGp1c3Q6IGV4YWN0Owp9CgpoMiB7Cglmb250LXNpemU6IDEuM2VtOwoJbGluZS1oZWlnaHQ6IDEuM2VtOwp9CgphIHsKCWNvbG9yOiAjMDAwOwoJdGV4dC1kZWNvcmF0aW9uOiBub25lOwp9Cg==">
	</head>
	<body>
		<ul class="report combineChildItems">
			<li id="item_4TURHWZD" class="item conferencePaper">
			<h2>Cloud-based multimodal human-robot interaction simulator utilizing ROS and unity frameworks</h2>
				<table>
					<tbody><tr>
						<th>Type</th>
						<td>Conference Paper</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yoshiaki Mizuchi</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Tetsunari Inamura</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>A new software architecture for a simulator of human-robot 
interaction (HRI) in virtual reality (VR) environments is proposed. 
Since collecting and storing a massive amount of data concerning 
multimodal interaction experiences is an important task concerning 
research on HRI, a cloud-based VR platform, named “SIGVerse,” which 
reduces costs of developing real robots and interaction experiments in 
the real world, is proposed. The reusability of virtual robot software 
is restricted in a real environment due to difference between VR and 
real robot architectures; therefore, a new architecture utilizing the 
Unity and ROS frameworks is proposed. The proposed architecture provides
 functionalities for constructing scalable 3D environments, embodied and
 social interaction via the Internet, compatible robot software, 
highﬁdelity sensor feedback, and recording/playback of interaction. To 
demonstrate the feasibility of the proposed architecture, the 
performance of SIGVerse in terms of simulating of multimodal information
 in actual interaction applications was evaluated, and the latency 
between avatars synchronized via cloud computing was measured. 
Additionally, interactive behaviors of robots and avatars in a VR 
environment and a real environment were experimentally compared, and the
 comparison results conﬁrm that the VR behaviors of robots and avatars 
were almost the same as the behavior of a robot in a real environment.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>12/2017</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>DOI.org (Crossref)</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://ieeexplore.ieee.org/document/8279345/">http://ieeexplore.ieee.org/document/8279345/</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/11/2021, 10:54:31 AM</td>
					</tr>
					<tr>
					<th>Place</th>
						<td>Taipei</td>
					</tr>
					<tr>
					<th>Publisher</th>
						<td>IEEE</td>
					</tr>
					<tr>
					<th>ISBN</th>
						<td>978-1-5386-2263-6</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>948-955</td>
					</tr>
					<tr>
					<th>Proceedings Title</th>
						<td>2017 IEEE/SICE International Symposium on System Integration (SII)</td>
					</tr>
					<tr>
					<th>Conference Name</th>
						<td>2017 IEEE/SICE International Symposium on System Integration (SII)</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1109/SII.2017.8279345">10.1109/SII.2017.8279345</a></td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>12/1/2021, 5:15:48 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>12/1/2021, 5:15:48 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_4KA3RSDE">Mizuchi and Inamura - 2017 - Cloud-based multimodal human-robot interaction sim.pdf					</li>
				</ul>
			</li>


			<li id="item_83HMHRWN" class="item journalArticle">
			<h2>Early Prototyping and Human Evaluation of Social Robot Navigation via Online Interactive Simulations</h2>
				<table>
					<tbody><tr>
						<th>Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Nathan Tsoi</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Marynel Vazquez</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>We recently created the open-source SEAN experimental platform
 (SEAN-EP). Leveraging the Social Environment for Autonomous Navigation 
(SEAN) simulator, we enable roboticists to collect human feedback for 
social robot navigation at scale. Our platform leverages modern web 
technologies to allow researchers to efﬁciently collect human feedback 
for social robot navigation tasks through interactive web forms. Remote 
users are able to control the motion of a human avatar via their web 
browser and interact with a virtual robot. Computation is delegated to 
cloud servers so users do not need specialized hardware to interact with
 the simulation environment. We provide an evaluation toolkit to allow 
fair comparison between robot navigation methods using common metrics. 
Usability was evaluated through an interactive online survey. Though 
there are many opportunities and challenges for this type of system, 
this is a promising step forward for evaluation of social navigation 
algorithms. We hope the community will beneﬁt from our open source 
platform.</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Zotero</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>6</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>12/1/2021, 5:15:48 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>12/1/2021, 5:15:48 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_TXRQVTUV">Tsoi and Vazquez - Early Prototyping and Human Evaluation of Social R.pdf					</li>
				</ul>
			</li>


			<li id="item_CYB4KIFU" class="item conferencePaper">
			<h2>Let’s Come Together — Social Navigation Behaviors of Virtual and Real Humans</h2>
				<table>
					<tbody><tr>
						<th>Type</th>
						<td>Conference Paper</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Matthias Rehm</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Elisabeth André</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Michael Nischt</td>
					</tr>
					<tr>
						<th class="editor">Editor</th>
						<td>Mark Maybury</td>
					</tr>
					<tr>
						<th class="editor">Editor</th>
						<td>Oliviero Stock</td>
					</tr>
					<tr>
						<th class="editor">Editor</th>
						<td>Wolfgang Wahlster</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>In this paper, we present a game-like scenario that is based 
on a model of social group dynamics inspired by theories from the social
 sciences. The model is augmented by a model of proxemics that simulates
 the role of distance and spatial orientation in human-human 
communication. By means of proxemics, a group of human participants may 
signal other humans whether they welcome new group members to join or 
not. In this paper, we describe the results of an experiment we 
conducted to shed light on the question of how humans respond to such 
cues when shown by virtual humans.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2005</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Springer Link</td>
					</tr>
					<tr>
					<th>Place</th>
						<td>Berlin, Heidelberg</td>
					</tr>
					<tr>
					<th>Publisher</th>
						<td>Springer</td>
					</tr>
					<tr>
					<th>ISBN</th>
						<td>978-3-540-31651-0</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>124-133</td>
					</tr>
					<tr>
					<th>Series</th>
						<td>Lecture Notes in Computer Science</td>
					</tr>
					<tr>
					<th>Proceedings Title</th>
						<td>Intelligent Technologies for Interactive Entertainment</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1007/11590323_13">10.1007/11590323_13</a></td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>12/1/2021, 5:15:48 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>12/1/2021, 5:15:48 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Social Force Model</li>
					<li>Real Human</li>
					<li>Social Distance</li>
					<li>Virtual Agent</li>
					<li>Virtual Human</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_48CIJ7C4">Springer Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_BUAWSERM" class="item conferencePaper">
			<h2>Mixed Reality for robotics</h2>
				<table>
					<tbody><tr>
						<th>Type</th>
						<td>Conference Paper</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>12/1/2021, 5:15:48 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>12/1/2021, 5:15:48 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_ZX9ZTEEE">2015_IROS_Hoenig.pdf					</li>
				</ul>
			</li>

		</ul>
	
</body></html>